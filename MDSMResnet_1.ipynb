{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNET Training for MDSM\n",
    "### 1. Instrall required pip package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade torchsummary\n",
    "!pip install --upgrade shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. import required python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import time, sys\n",
    "\n",
    "from ResNet import Bottleneck, ResNet, ResNet50, ResNet18, ResNet34, ResNet101, ResNet152\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "mdsm_width = 11\n",
    "mdsm_height = 108"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define MDSMdata set for torch Dataset and Dataloader and other\n",
    "* mix_random : For preventing overfitting (Mixing row randomly in MDSM metric\n",
    "* flip_random : For preventing overfitting (Flip MDSM upside-down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_random(col, row, mdsm_body):\n",
    "    size_suffle = random.randint(0,10)\n",
    "    switchsource = torch.randint(0, row - 1, (size_suffle,))\n",
    "    temp = np.zeros((1, col), np.float32)\n",
    "    \n",
    "    for i in range(0, int(size_suffle)):\n",
    "        if i == switchsource[i]:\n",
    "            continue\n",
    "        temp = mdsm_body[i, :].copy()\n",
    "        mdsm_body[i, :] = mdsm_body[switchsource[i], :].copy()\n",
    "        mdsm_body[switchsource[i], :] = temp.copy()\n",
    "    return torch.tensor(mdsm_body)\n",
    "\n",
    "def flip_random(col, row, mdsm_body):\n",
    "    size_suffle = random.randint(0,12)\n",
    "    if size_suffle % 4 != 0:\n",
    "        return torch.tensor(mdsm_body)\n",
    "    \n",
    "    int_row = int(row)\n",
    "    for i in range(0, int(int_row / 2)):\n",
    "        temp = mdsm_body[i, :].copy()\n",
    "        mdsm_body[i, :] = mdsm_body[int_row - i - 1, :].copy()\n",
    "        mdsm_body[int_row - i - 1, :] = temp.copy()\n",
    "    return torch.tensor(mdsm_body)\n",
    "    \n",
    "class MDSMDataset(Dataset):\n",
    "    def __init__(self, mdsmdata_file):\n",
    "        self.df = pd.read_csv(mdsmdata_file)\n",
    "        rating = self.df[['ReviewID', 'reviewStar']]\n",
    "        self.rating = rating.drop_duplicates('ReviewID')\n",
    "        self.height = self.df['ReviewID'].value_counts().max()\n",
    "\n",
    "        mdsm_body = self.df.drop(['reviewNo', 'reviewStar', 'mGNR'], axis=1)\n",
    "        mdsm_body['imageCnt'] = (mdsm_body['imageCnt'] - mdsm_body['imageCnt'].min())/ (mdsm_body['imageCnt'].max() - mdsm_body['imageCnt'].min())\n",
    "        mdsm_body['helpfulCnt'] = (mdsm_body['helpfulCnt'] - mdsm_body['helpfulCnt'].mean())/ mdsm_body['helpfulCnt'].std()\n",
    "        body_height, body_width = mdsm_body.shape;\n",
    "        self.width = body_width - 1\n",
    "        mdsm_width = self.width\n",
    "        mdsm_height = self.height\n",
    "\n",
    "        dummy_mdsd = np.zeros((body_height, self.height, self.width), np.float32)\n",
    "        mdsm_index = np.zeros(self.rating['ReviewID'].max()+1, int)\n",
    "        mdsm_count = np.zeros(self.rating['ReviewID'].max()+1, int)\n",
    "        mdsm_index.fill(-1)\n",
    "\n",
    "        max_index = int(0)\n",
    "        for index, body in mdsm_body.iterrows():\n",
    "            dummy_index = max_index\n",
    "            if mdsm_index[int(body['ReviewID'])] != -1:\n",
    "                dummy_index = mdsm_index[int(body['ReviewID'])]\n",
    "            else:\n",
    "                mdsm_index[int(body['ReviewID'])] = dummy_index\n",
    "                max_index = max_index + 1\n",
    "\n",
    "            dummy_mdsd[dummy_index, mdsm_count[dummy_index]] = body.drop('ReviewID')\n",
    "            mdsm_count[dummy_index] = mdsm_count[dummy_index] + 1\n",
    "\n",
    "        self.mdsm_body = dummy_mdsd\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.rating.shape[0]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if trans_stat == True:\n",
    "            _tensor = flip_random(self.width, self.height, self.mdsm_body[idx])\n",
    "        else:\n",
    "            _tensor = torch.tensor(self.mdsm_body[idx])\n",
    "        rtn_tensor = _tensor.unsqueeze(0)\n",
    "        return rtn_tensor, self.rating.iloc[idx, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter setting\n",
    "* epochs, batch_size ResNet layer number\n",
    "* select ResNet model\n",
    "* print out Model strucuture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512\n",
    "net_type = \"ResNet18\"\n",
    "\n",
    "if net_type == \"ResNet18\":\n",
    "    net = ResNet18(6, 1).to('cuda')\n",
    "    print(\"ResNet18 is used\")\n",
    "elif net_type == \"ResNet34\":\n",
    "    net = ResNet34(6, 1).to('cuda')\n",
    "    print(\"ResNet34 is used\")\n",
    "elif net_type == \"ResNet50\":\n",
    "    net = ResNet50(6, 1).to('cuda')\n",
    "    print(\"ResNet50 is used\")\n",
    "elif net_type == \"ResNet101\":\n",
    "    net = ResNet101(6, 1).to('cuda')\n",
    "    print(\"ResNet101 is used\")\n",
    "elif net_type == \"ResNet152\":\n",
    "    net = ResNet152(6, 1).to('cuda')\n",
    "    print(\"ResNet152 is used\")\n",
    "\n",
    "summary(net, (1, mdsm_height, mdsm_width))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load MDSM dataset from preprocesed csv file\n",
    "* split train and test dataset into 8:2 ratio\n",
    "* 6 kinds of classess [0, 1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('-- Loading dataset--')\n",
    "\n",
    "dataset = MDSMDataset('amazon_hmdvr_df_tokenized_sentiment_score_extended_normalized.csv')\n",
    "train_size = round(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "print(\"Train(\", train_size, \") vs Test(\", test_size, \")\")\n",
    "\n",
    "print('-- Building train and test dataset / dataloader--')\n",
    "train_dataset, test_dataset = random_split(dataset, [int(train_size),int(test_size)])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "classes = ['0', '1', '2', '3', '4', '5']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define mae and mse calcuating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcu_metric(outputs, labels):\n",
    "    mae = abs(outputs - labels)\n",
    "    mse = torch.sqrt(mae)\n",
    "    return mae, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Do ResNet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "trans_stat = True\n",
    "print('-- Start training : ', EPOCHS, 'epochs')\n",
    "start = time.time()\n",
    "\n",
    "mse_history = {'train': [], 'val': []}\n",
    "mae_history = {'train': [], 'val': []}\n",
    "loss_history = {'train': [], 'val': []}\n",
    "acc_history = {'train': [], 'val': []}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    \n",
    "    mse = np.zeros(train_size, np.float32)\n",
    "    mae = np.zeros(train_size, np.float32)\n",
    "    \n",
    "    \n",
    "    val_mse = np.zeros(test_size, np.float32)\n",
    "    val_mae = np.zeros(test_size, np.float32)\n",
    "    \n",
    "    metric_index = 0\n",
    "    \n",
    "    running_loss = 0\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for i, inp in enumerate(trainloader):\n",
    "        inputs, labels = inp\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        train_acc += pred.eq(labels.data.view_as(pred)).sum()\n",
    "        \n",
    "\n",
    "        _mae, _mse = calcu_metric(pred.squeeze(), labels)\n",
    "        #print(_mse.detach().cpu().numpy())\n",
    "        mae[metric_index:metric_index+len(inputs)] = _mae.detach().cpu().numpy()\n",
    "        mse[metric_index:metric_index+len(inputs)] = _mse.detach().cpu().numpy()\n",
    "        #print(torch.tensor(mse))\n",
    "        metric_index += len(inputs)\n",
    "        \n",
    "        if i%100 == 0 and i > 0:\n",
    "            print(f'Loss [{epoch+1}/{EPOCHS}, {i}](epoch, minibatch): ', f'{running_loss / 100:.5f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    mae_epoch = torch.mean(torch.tensor(mae))\n",
    "    mse_epoch = torch.mean(torch.tensor(mse))\n",
    "    \n",
    "    mse_history['train'].append(mse_epoch)\n",
    "    mae_history['train'].append(mae_epoch)\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    metric_index = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to('cuda'), labels.to('cuda')\n",
    "            outputs = net(images)\n",
    "\n",
    "            pred = outputs.data.max(1, keepdim=True)[1]\n",
    "            _mae, _mse = calcu_metric(pred.squeeze(), labels)\n",
    "            val_mae[metric_index:metric_index+len(images)] = _mae.detach().cpu().numpy()\n",
    "            val_mse[metric_index:metric_index+len(images)] = _mse.detach().cpu().numpy()\n",
    "            metric_index += len(images)           \n",
    "        \n",
    "    val_mae_epoch = torch.mean(torch.tensor(val_mae))\n",
    "    val_mse_epoch = torch.mean(torch.tensor(val_mse))\n",
    "    mse_history['val'].append(val_mse_epoch)\n",
    "    mae_history['val'].append(val_mae_epoch)\n",
    "    acc_history['train'].append((100. * train_acc / len(trainloader.dataset)).detach().cpu().numpy())\n",
    "    \n",
    "    train_loss /= len(trainloader.dataset)\n",
    "    if EPOCHS > 50:\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: {}/{} Avg. loss:{:.4f} Acc.: {:.4f}% '.format(epoch, EPOCHS, train_loss, 100. * train_acc / len(trainloader.dataset)), f\"MAE : {mae_epoch.item():.3f}\", f\"MSE : {mse_epoch.item():.3f}\", f\"VAL_MAE : {val_mae_epoch.item():.3f}\", f\"VAL_MSE : {val_mse_epoch.item():.3f}\" \")\")\n",
    "    else:\n",
    "        print('Epoch: {}/{} Avg. loss:{:.4f} Acc.: {:.4f}% '.format(epoch, EPOCHS, train_loss, 100. * train_acc / len(trainloader.dataset)), f\"MAE : {mae_epoch.item():.3f}\", f\"MSE : {mse_epoch.item():.3f}\", f\"VAL_MAE : {val_mae_epoch.item():.3f}\", f\"VAL_MSE : {val_mse_epoch.item():.3f}\" \")\")\n",
    "\n",
    "print('Training Done')\n",
    "trans_stat = False\n",
    "end = time.time()\n",
    "print(f\"{net_type} training takes {end - start:.5f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Save model metrics into csv file\n",
    "* Hyperparameter were written in filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now(timezone('Asia/Seoul'))\n",
    "\n",
    "hist_csv = np.stack([mae_history['val'], mse_history['val'], mae_history['train'], \n",
    "                     mse_history['train'], acc_history['train']], 1)\n",
    "hist_csv_df = pd.DataFrame(hist_csv)\n",
    "hist_csv_df.columns = ['validation_mae', 'validation_mse', 'train_mae', 'train_mse', 'train_accuracy']\n",
    "time_str = now.strftime('%Y-%m-%d_%H:%M:%S')\n",
    "hist_csv_df.to_csv(\"training_metrics/amazon_hmdvr_df_tokenized_sentiment_score_model-{}_epochs-{}-batch-{}-{}.csv\"\n",
    "                   .format(net_type, EPOCHS, BATCH_SIZE, time_str), index=False)\n",
    "print(\"training_metrics/amazon_hmdvr_df_tokenized_sentiment_score_model-{}_epochs-{}-batch-{}-{}.csv saved\"\n",
    "                   .format(net_type, EPOCHS, BATCH_SIZE, time_str))\n",
    "\n",
    "torch.save({\n",
    "                \"epoch\": EPOCHS,\n",
    "                \"model_state_dict\": net.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "           \"check_points/amazon_hmdvr_df_tokenized_sentiment_score_model-{}_epochs-{}-batch-{}-{}.pt\"\n",
    "                   .format(net_type, EPOCHS, BATCH_SIZE, time_str))\n",
    "torch.save({\n",
    "                \"epoch\": EPOCHS,\n",
    "                \"model_state_dict\": net.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            },\"check_points/latest.pt\")\n",
    "torch.save(net, \"check_points/amazon_hmdvr_df_tokenized_sentiment_score_model-{}_epochs-{}-batch-{}-{}.model\"\n",
    "                   .format(net_type, EPOCHS, BATCH_SIZE, time_str))\n",
    "torch.save(net, \"check_points/latest.model\")\n",
    "\n",
    "print(\"check_points/amazon_hmdvr_df_tokenized_sentiment_score_model-{}_epochs-{}-batch-{}-{}.pt & check_points/latest.pt saved\"\n",
    "                   .format(net_type, EPOCHS, BATCH_SIZE, time_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Plot model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Mean Abs Error [STR] : model-{}_epochs-{}_batch-{}-{}\"\n",
    "                   .format(net_type, EPOCHS, BATCH_SIZE, time_str))\n",
    "plt.plot(range(1,EPOCHS+1),mae_history[\"train\"],label=\"train_mae\")\n",
    "plt.plot(range(1,EPOCHS+1),mae_history[\"val\"],label=\"validation_mae\")\n",
    "plt.ylabel(\"Mean Abs Error [STR]\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylim([0,1.5])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Mean Square Error [$STR^2$] : model-{}_epochs-{}_batch-{}-{}\"\n",
    "                   .format(net_type, EPOCHS, BATCH_SIZE, time_str))\n",
    "plt.plot(range(1,EPOCHS+1),mse_history[\"train\"],label=\"train_mse\")\n",
    "plt.plot(range(1,EPOCHS+1),mse_history[\"val\"],label=\"validation_mse\")\n",
    "plt.ylabel(\"Mean Square Error [$STR^2$]\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylim([0,1.5])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.title(\"Train Accuracy : model-{}_epochs-{}_batch-{}-{}\"\n",
    "                   .format(net_type, EPOCHS, BATCH_SIZE, time_str))\n",
    "plt.plot(range(1,EPOCHS+1),acc_history[\"train\"],label=\"train_accuracy\")\n",
    "#plt.plot(range(1,EPOCHS+1),mse_history[\"val\"],label=\"validation mse\")\n",
    "plt.ylabel(\"Train Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "#plt.ylim([0,1.5])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Using SHAP for XAI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Shap command has to been excuted by manually after training finishing(need time gap)___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(testloader))\n",
    "images, _ = batch\n",
    "\n",
    "#max_size = BATCH_SIZE - 3\n",
    "max_size = 100\n",
    "shap_test_size = max_size + 3\n",
    "\n",
    "background = images[:max_size]\n",
    "test_images = images[max_size:shap_test_size]\n",
    "\n",
    "#e = shap.DeepExplainer(net, background.to(device))\n",
    "e = shap.DeepExplainer(net, background.to(device))\n",
    "shap_values = e.shap_values(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('amazon_hmdvr_df_tokenized_sentiment_score_extended_normalized.csv')\n",
    "dff = df.drop(['reviewNo', 'ReviewID', 'reviewStar', 'mGNR'], axis=1)\n",
    "\n",
    "shap.initjs()\n",
    "shap.summary_plot(shap_values[0][0][0], images[:][0][0], feature_names=dff.columns)\n",
    "shap.summary_plot(shap_values[0][0][0], images[:][0][0], feature_names=dff.columns,plot_type='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
