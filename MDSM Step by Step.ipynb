{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.1+cu116)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision) (2.22.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.23.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already up-to-date: pandas in /usr/local/lib/python3.8/dist-packages (1.5.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.20.3; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already up-to-date: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "!pip install --upgrade pandas\n",
    "!pip install --upgrade torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ResNet import Bottleneck, ResNet, ResNet50\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import models\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, out_planes, i_downsample=None, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # stride를 통해 너비와 높이 조정\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        \n",
    "        # stride = 1, padding = 1이므로, 너비와 높이는 항시 유지됨\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        \n",
    "        # x를 그대로 더해주기 위함\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.i_downsample = i_downsample\n",
    "        \n",
    "        \n",
    "        # 만약 size가 안맞아 합연산이 불가하다면, 연산 가능하도록 모양을 맞춰줌\n",
    "        if stride != 1: # x와 \n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(x) # 필요에 따라 layer를 Skip\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "def ResNet18(num_classes, channels=3):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes, channels)\n",
    "\n",
    "def ResNet34(num_classes, channels=3):\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDSMDataset(Dataset):\n",
    "    def __init__(self, mdsmdata_file):\n",
    "        self.df = pd.read_csv(mdsmdata_file)\n",
    "        rating = self.df[['ReviewID', 'reviewStar']]\n",
    "        self.rating = rating.drop_duplicates('ReviewID')\n",
    "        self.height = self.df['ReviewID'].value_counts().max()\n",
    "\n",
    "        mdsm_body = self.df.drop(['reviewNo', 'reviewStar'], axis=1)\n",
    "        mdsm_body['imageCnt'] = (mdsm_body['imageCnt'] - mdsm_body['imageCnt'].min())/ (mdsm_body['imageCnt'].max() - mdsm_body['imageCnt'].min())\n",
    "        mdsm_body['helpfulCnt'] = (mdsm_body['helpfulCnt'] - mdsm_body['helpfulCnt'].mean())/ mdsm_body['helpfulCnt'].std()\n",
    "        body_height, body_width = mdsm_body.shape;\n",
    "        self.width = body_width - 1\n",
    "\n",
    "        dummy_mdsd = np.zeros((body_height, self.height, self.width), np.float32)\n",
    "        mdsm_index = np.zeros(self.rating['ReviewID'].max()+1, int)\n",
    "        mdsm_count = np.zeros(self.rating['ReviewID'].max()+1, int)\n",
    "        mdsm_index.fill(-1)\n",
    "\n",
    "        max_index = int(0)\n",
    "        for index, body in mdsm_body.iterrows():\n",
    "            dummy_index = max_index\n",
    "            if mdsm_index[int(body['ReviewID'])] != -1:\n",
    "                dummy_index = mdsm_index[int(body['ReviewID'])]\n",
    "            else:\n",
    "                mdsm_index[int(body['ReviewID'])] = dummy_index\n",
    "                max_index = max_index + 1\n",
    "\n",
    "            dummy_mdsd[dummy_index, mdsm_count[dummy_index]] = body.drop('ReviewID')\n",
    "            mdsm_count[dummy_index] = mdsm_count[dummy_index] + 1\n",
    "\n",
    "        self.mdsm_body = dummy_mdsd\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.rating.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _tensor = torch.tensor(self.mdsm_body[idx])\n",
    "        rtn_tensor = _tensor.unsqueeze(0)\n",
    "        return rtn_tensor, self.rating.iloc[idx, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Loading dataset--\n",
      "-- Building train and test dataset / dataloader--\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 64, 54, 6]           3,136\n",
      "       BatchNorm2d-2            [-1, 64, 54, 6]             128\n",
      "              ReLU-3            [-1, 64, 54, 6]               0\n",
      "         MaxPool2d-4            [-1, 64, 27, 3]               0\n",
      "            Conv2d-5            [-1, 64, 27, 3]          36,864\n",
      "       BatchNorm2d-6            [-1, 64, 27, 3]             128\n",
      "            Conv2d-7            [-1, 64, 27, 3]          36,864\n",
      "       BatchNorm2d-8            [-1, 64, 27, 3]             128\n",
      "        BasicBlock-9            [-1, 64, 27, 3]               0\n",
      "           Conv2d-10            [-1, 64, 27, 3]          36,864\n",
      "      BatchNorm2d-11            [-1, 64, 27, 3]             128\n",
      "           Conv2d-12            [-1, 64, 27, 3]          36,864\n",
      "      BatchNorm2d-13            [-1, 64, 27, 3]             128\n",
      "       BasicBlock-14            [-1, 64, 27, 3]               0\n",
      "           Conv2d-15            [-1, 64, 27, 3]          36,864\n",
      "      BatchNorm2d-16            [-1, 64, 27, 3]             128\n",
      "           Conv2d-17            [-1, 64, 27, 3]          36,864\n",
      "      BatchNorm2d-18            [-1, 64, 27, 3]             128\n",
      "       BasicBlock-19            [-1, 64, 27, 3]               0\n",
      "           Conv2d-20           [-1, 128, 14, 2]          73,728\n",
      "      BatchNorm2d-21           [-1, 128, 14, 2]             256\n",
      "           Conv2d-22           [-1, 128, 14, 2]         147,456\n",
      "      BatchNorm2d-23           [-1, 128, 14, 2]             256\n",
      "           Conv2d-24           [-1, 128, 14, 2]           8,192\n",
      "      BatchNorm2d-25           [-1, 128, 14, 2]             256\n",
      "       BasicBlock-26           [-1, 128, 14, 2]               0\n",
      "           Conv2d-27           [-1, 128, 14, 2]         147,456\n",
      "      BatchNorm2d-28           [-1, 128, 14, 2]             256\n",
      "           Conv2d-29           [-1, 128, 14, 2]         147,456\n",
      "      BatchNorm2d-30           [-1, 128, 14, 2]             256\n",
      "       BasicBlock-31           [-1, 128, 14, 2]               0\n",
      "           Conv2d-32           [-1, 128, 14, 2]         147,456\n",
      "      BatchNorm2d-33           [-1, 128, 14, 2]             256\n",
      "           Conv2d-34           [-1, 128, 14, 2]         147,456\n",
      "      BatchNorm2d-35           [-1, 128, 14, 2]             256\n",
      "       BasicBlock-36           [-1, 128, 14, 2]               0\n",
      "           Conv2d-37           [-1, 128, 14, 2]         147,456\n",
      "      BatchNorm2d-38           [-1, 128, 14, 2]             256\n",
      "           Conv2d-39           [-1, 128, 14, 2]         147,456\n",
      "      BatchNorm2d-40           [-1, 128, 14, 2]             256\n",
      "       BasicBlock-41           [-1, 128, 14, 2]               0\n",
      "           Conv2d-42            [-1, 256, 7, 1]         294,912\n",
      "      BatchNorm2d-43            [-1, 256, 7, 1]             512\n",
      "           Conv2d-44            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-45            [-1, 256, 7, 1]             512\n",
      "           Conv2d-46            [-1, 256, 7, 1]          32,768\n",
      "      BatchNorm2d-47            [-1, 256, 7, 1]             512\n",
      "       BasicBlock-48            [-1, 256, 7, 1]               0\n",
      "           Conv2d-49            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-50            [-1, 256, 7, 1]             512\n",
      "           Conv2d-51            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-52            [-1, 256, 7, 1]             512\n",
      "       BasicBlock-53            [-1, 256, 7, 1]               0\n",
      "           Conv2d-54            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-55            [-1, 256, 7, 1]             512\n",
      "           Conv2d-56            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-57            [-1, 256, 7, 1]             512\n",
      "       BasicBlock-58            [-1, 256, 7, 1]               0\n",
      "           Conv2d-59            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-60            [-1, 256, 7, 1]             512\n",
      "           Conv2d-61            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-62            [-1, 256, 7, 1]             512\n",
      "       BasicBlock-63            [-1, 256, 7, 1]               0\n",
      "           Conv2d-64            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-65            [-1, 256, 7, 1]             512\n",
      "           Conv2d-66            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-67            [-1, 256, 7, 1]             512\n",
      "       BasicBlock-68            [-1, 256, 7, 1]               0\n",
      "           Conv2d-69            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-70            [-1, 256, 7, 1]             512\n",
      "           Conv2d-71            [-1, 256, 7, 1]         589,824\n",
      "      BatchNorm2d-72            [-1, 256, 7, 1]             512\n",
      "       BasicBlock-73            [-1, 256, 7, 1]               0\n",
      "           Conv2d-74            [-1, 512, 4, 1]       1,179,648\n",
      "      BatchNorm2d-75            [-1, 512, 4, 1]           1,024\n",
      "           Conv2d-76            [-1, 512, 4, 1]       2,359,296\n",
      "      BatchNorm2d-77            [-1, 512, 4, 1]           1,024\n",
      "           Conv2d-78            [-1, 512, 4, 1]         131,072\n",
      "      BatchNorm2d-79            [-1, 512, 4, 1]           1,024\n",
      "       BasicBlock-80            [-1, 512, 4, 1]               0\n",
      "           Conv2d-81            [-1, 512, 4, 1]       2,359,296\n",
      "      BatchNorm2d-82            [-1, 512, 4, 1]           1,024\n",
      "           Conv2d-83            [-1, 512, 4, 1]       2,359,296\n",
      "      BatchNorm2d-84            [-1, 512, 4, 1]           1,024\n",
      "       BasicBlock-85            [-1, 512, 4, 1]               0\n",
      "           Conv2d-86            [-1, 512, 4, 1]       2,359,296\n",
      "      BatchNorm2d-87            [-1, 512, 4, 1]           1,024\n",
      "           Conv2d-88            [-1, 512, 4, 1]       2,359,296\n",
      "      BatchNorm2d-89            [-1, 512, 4, 1]           1,024\n",
      "       BasicBlock-90            [-1, 512, 4, 1]               0\n",
      "AdaptiveAvgPool2d-91            [-1, 512, 1, 1]               0\n",
      "           Linear-92                    [-1, 6]           3,078\n",
      "================================================================\n",
      "Total params: 21,281,478\n",
      "Trainable params: 21,281,478\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.42\n",
      "Params size (MB): 81.18\n",
      "Estimated Total Size (MB): 83.60\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-- Loading dataset--')\n",
    "dataset = MDSMDataset('amazon_hmdvr_df_tokenized_sentiment_score_extended.csv')\n",
    "\n",
    "train_size = len(dataset) * 0.8\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "print('-- Building train and test dataset / dataloader--')\n",
    "train_dataset, test_dataset = random_split(dataset, [int(train_size),int(test_size)])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = 64, shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = 64, shuffle=True, num_workers=0)\n",
    "\n",
    "classes = ['0', '1', '2', '3', '4', '5']\n",
    "\n",
    "#classes = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "#net = ResNet50(6, 1).to('cuda')\n",
    "#net = ResNet18(6, 1).to('cuda')\n",
    "net = ResNet34(6, 1).to('cuda')\n",
    "\n",
    "summary(net, (1, 108, 12))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Start training :  100 epochs\n",
      "Loss [1/100, 100](epoch, minibatch):  1.94568\n",
      "Loss [1/100, 200](epoch, minibatch):  1.24062\n",
      "Train Epoch: 0 Average loss: 0.0248 Accuracy : 56.7182%)\n",
      "Loss [2/100, 100](epoch, minibatch):  1.31591\n",
      "Loss [2/100, 200](epoch, minibatch):  1.22624\n",
      "Train Epoch: 1 Average loss: 0.0200 Accuracy : 57.1125%)\n",
      "Loss [3/100, 100](epoch, minibatch):  1.32042\n",
      "Loss [3/100, 200](epoch, minibatch):  1.18556\n",
      "Train Epoch: 2 Average loss: 0.0197 Accuracy : 57.5751%)\n",
      "Loss [4/100, 100](epoch, minibatch):  1.25675\n",
      "Loss [4/100, 200](epoch, minibatch):  1.17552\n",
      "Train Epoch: 3 Average loss: 0.0194 Accuracy : 56.9305%)\n",
      "Loss [5/100, 100](epoch, minibatch):  1.18340\n",
      "Loss [5/100, 200](epoch, minibatch):  1.15635\n",
      "Train Epoch: 4 Average loss: 0.0183 Accuracy : 59.2812%)\n",
      "Loss [6/100, 100](epoch, minibatch):  1.14278\n",
      "Loss [6/100, 200](epoch, minibatch):  1.14905\n",
      "Train Epoch: 5 Average loss: 0.0178 Accuracy : 60.2138%)\n",
      "Loss [7/100, 100](epoch, minibatch):  1.14985\n",
      "Loss [7/100, 200](epoch, minibatch):  1.12521\n",
      "Train Epoch: 6 Average loss: 0.0177 Accuracy : 60.3579%)\n",
      "Loss [8/100, 100](epoch, minibatch):  1.13272\n",
      "Loss [8/100, 200](epoch, minibatch):  1.11786\n",
      "Train Epoch: 7 Average loss: 0.0176 Accuracy : 61.5180%)\n",
      "Loss [9/100, 100](epoch, minibatch):  1.12089\n",
      "Loss [9/100, 200](epoch, minibatch):  1.10398\n",
      "Train Epoch: 8 Average loss: 0.0174 Accuracy : 61.9730%)\n",
      "Loss [10/100, 100](epoch, minibatch):  1.13358\n",
      "Loss [10/100, 200](epoch, minibatch):  1.09352\n",
      "Train Epoch: 9 Average loss: 0.0174 Accuracy : 61.7910%)\n",
      "Loss [11/100, 100](epoch, minibatch):  1.11068\n",
      "Loss [11/100, 200](epoch, minibatch):  1.10376\n",
      "Train Epoch: 10 Average loss: 0.0173 Accuracy : 62.2839%)\n",
      "Loss [12/100, 100](epoch, minibatch):  1.08351\n",
      "Loss [12/100, 200](epoch, minibatch):  1.10267\n",
      "Train Epoch: 11 Average loss: 0.0171 Accuracy : 62.6858%)\n",
      "Loss [13/100, 100](epoch, minibatch):  1.13959\n",
      "Loss [13/100, 200](epoch, minibatch):  1.10479\n",
      "Train Epoch: 12 Average loss: 0.0174 Accuracy : 61.2527%)\n",
      "Loss [14/100, 100](epoch, minibatch):  1.10786\n",
      "Loss [14/100, 200](epoch, minibatch):  1.08800\n",
      "Train Epoch: 13 Average loss: 0.0172 Accuracy : 62.2687%)\n",
      "Loss [15/100, 100](epoch, minibatch):  1.09693\n",
      "Loss [15/100, 200](epoch, minibatch):  1.08465\n",
      "Train Epoch: 14 Average loss: 0.0171 Accuracy : 62.4962%)\n",
      "Loss [16/100, 100](epoch, minibatch):  1.08784\n",
      "Loss [16/100, 200](epoch, minibatch):  1.08737\n",
      "Train Epoch: 15 Average loss: 0.0170 Accuracy : 62.5645%)\n",
      "Loss [17/100, 100](epoch, minibatch):  1.09986\n",
      "Loss [17/100, 200](epoch, minibatch):  1.06928\n",
      "Train Epoch: 16 Average loss: 0.0169 Accuracy : 62.6479%)\n",
      "Loss [18/100, 100](epoch, minibatch):  1.07372\n",
      "Loss [18/100, 200](epoch, minibatch):  1.07443\n",
      "Train Epoch: 17 Average loss: 0.0167 Accuracy : 63.0497%)\n",
      "Loss [19/100, 100](epoch, minibatch):  1.08391\n",
      "Loss [19/100, 200](epoch, minibatch):  1.07403\n",
      "Train Epoch: 18 Average loss: 0.0168 Accuracy : 62.7919%)\n",
      "Loss [20/100, 100](epoch, minibatch):  1.06695\n",
      "Loss [20/100, 200](epoch, minibatch):  1.06672\n",
      "Train Epoch: 19 Average loss: 0.0166 Accuracy : 63.1256%)\n",
      "Loss [21/100, 100](epoch, minibatch):  1.06761\n",
      "Loss [21/100, 200](epoch, minibatch):  1.06689\n",
      "Train Epoch: 20 Average loss: 0.0167 Accuracy : 62.9891%)\n",
      "Loss [22/100, 100](epoch, minibatch):  1.05965\n",
      "Loss [22/100, 200](epoch, minibatch):  1.06217\n",
      "Train Epoch: 21 Average loss: 0.0165 Accuracy : 62.9815%)\n",
      "Loss [23/100, 100](epoch, minibatch):  1.05719\n",
      "Loss [23/100, 200](epoch, minibatch):  1.06080\n",
      "Train Epoch: 22 Average loss: 0.0165 Accuracy : 63.1938%)\n",
      "Loss [24/100, 100](epoch, minibatch):  1.05531\n",
      "Loss [24/100, 200](epoch, minibatch):  1.06119\n",
      "Train Epoch: 23 Average loss: 0.0165 Accuracy : 63.3151%)\n",
      "Loss [25/100, 100](epoch, minibatch):  1.06967\n",
      "Loss [25/100, 200](epoch, minibatch):  1.05348\n",
      "Train Epoch: 24 Average loss: 0.0166 Accuracy : 63.2545%)\n",
      "Loss [26/100, 100](epoch, minibatch):  1.07041\n",
      "Loss [26/100, 200](epoch, minibatch):  1.05085\n",
      "Train Epoch: 25 Average loss: 0.0166 Accuracy : 63.1559%)\n",
      "Loss [27/100, 100](epoch, minibatch):  1.07069\n",
      "Loss [27/100, 200](epoch, minibatch):  1.03713\n",
      "Train Epoch: 26 Average loss: 0.0165 Accuracy : 63.4289%)\n",
      "Loss [28/100, 100](epoch, minibatch):  1.05174\n",
      "Loss [28/100, 200](epoch, minibatch):  1.05323\n",
      "Train Epoch: 27 Average loss: 0.0164 Accuracy : 63.4971%)\n",
      "Loss [29/100, 100](epoch, minibatch):  1.05263\n",
      "Loss [29/100, 200](epoch, minibatch):  1.04579\n",
      "Train Epoch: 28 Average loss: 0.0164 Accuracy : 63.4820%)\n",
      "Loss [30/100, 100](epoch, minibatch):  1.06308\n",
      "Loss [30/100, 200](epoch, minibatch):  1.03421\n",
      "Train Epoch: 29 Average loss: 0.0163 Accuracy : 63.5881%)\n",
      "Loss [31/100, 100](epoch, minibatch):  1.04791\n",
      "Loss [31/100, 200](epoch, minibatch):  1.04273\n",
      "Train Epoch: 30 Average loss: 0.0163 Accuracy : 63.3530%)\n",
      "Loss [32/100, 100](epoch, minibatch):  1.05029\n",
      "Loss [32/100, 200](epoch, minibatch):  1.03959\n",
      "Train Epoch: 31 Average loss: 0.0163 Accuracy : 63.4213%)\n",
      "Loss [33/100, 100](epoch, minibatch):  1.05361\n",
      "Loss [33/100, 200](epoch, minibatch):  1.03537\n",
      "Train Epoch: 32 Average loss: 0.0163 Accuracy : 63.5502%)\n",
      "Loss [34/100, 100](epoch, minibatch):  1.03529\n",
      "Loss [34/100, 200](epoch, minibatch):  1.04950\n",
      "Train Epoch: 33 Average loss: 0.0162 Accuracy : 63.5805%)\n",
      "Loss [35/100, 100](epoch, minibatch):  1.04129\n",
      "Loss [35/100, 200](epoch, minibatch):  1.04048\n",
      "Train Epoch: 34 Average loss: 0.0162 Accuracy : 63.6033%)\n",
      "Loss [36/100, 100](epoch, minibatch):  1.05060\n",
      "Loss [36/100, 200](epoch, minibatch):  1.03417\n",
      "Train Epoch: 35 Average loss: 0.0162 Accuracy : 63.6564%)\n",
      "Loss [37/100, 100](epoch, minibatch):  1.04063\n",
      "Loss [37/100, 200](epoch, minibatch):  1.03608\n",
      "Train Epoch: 36 Average loss: 0.0162 Accuracy : 63.7625%)\n",
      "Loss [38/100, 100](epoch, minibatch):  1.05798\n",
      "Loss [38/100, 200](epoch, minibatch):  1.02310\n",
      "Train Epoch: 37 Average loss: 0.0162 Accuracy : 63.9066%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 22\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m pred \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "EPOCHS = 100\n",
    "print('-- Start training : ', EPOCHS, 'epochs')\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    running_loss = 0\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for i, inp in enumerate(trainloader):\n",
    "        inputs, labels = inp\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        train_acc += pred.eq(labels.data.view_as(pred)).sum()\n",
    "\n",
    "        if i%100 == 0 and i > 0:\n",
    "            print(f'Loss [{epoch+1}/{EPOCHS}, {i}](epoch, minibatch): ', f'{running_loss / 100:.5f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    train_loss /= len(trainloader.dataset)\n",
    "    print('Train Epoch: {} Average loss: {:.4f} Accuracy : {:.4f}%)'.format(epoch, train_loss, 100. * train_acc / len(trainloader.dataset)))\n",
    "\n",
    "print('Training Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print('Accuracy on 10,000 test images: ', f'{100*(correct/total):.3f}', '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print(outputs.data)\n",
    "        print(predicted)\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
